#!/usr/bin/env python3
"""
éŸ³æ¥½åˆ†æå°‚é–€ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
generated-music.wavãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°åˆ†æã‚’å®Ÿè¡Œ
"""

import numpy as np
import librosa
import scipy.signal
from scipy import stats
import matplotlib.pyplot as plt
import os
import json
from datetime import datetime

def analyze_music_structure(y, sr, hop_length=512):
    """éŸ³æ¥½ã®æ§‹é€ åˆ†æï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²ï¼‰"""
    # ã‚¯ãƒ­ãƒã‚°ãƒ©ãƒ ã¨MFCCç‰¹å¾´é‡ã§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå¢ƒç•Œã‚’æ¤œå‡º
    chroma = librosa.feature.chroma(y=y, sr=sr, hop_length=hop_length)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)
    
    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå¢ƒç•Œã®æ¤œå‡º
    boundaries = librosa.segment.agglomerative(chroma, k=4)
    boundary_times = librosa.frames_to_time(boundaries, sr=sr, hop_length=hop_length)
    
    return boundary_times

def analyze_tempo_and_rhythm(y, sr):
    """ãƒ†ãƒ³ãƒã¨ãƒªã‚ºãƒ åˆ†æ"""
    # ãƒ†ãƒ³ãƒæ¤œå‡º
    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
    beat_times = librosa.frames_to_time(beats, sr=sr)
    
    # ãƒªã‚ºãƒ ã®å®‰å®šæ€§åˆ†æ
    beat_intervals = np.diff(beat_times)
    rhythm_stability = 1 - np.std(beat_intervals) / np.mean(beat_intervals) if len(beat_intervals) > 0 else 0
    
    return tempo, beat_times, rhythm_stability

def analyze_spectral_features(y, sr):
    """ã‚¹ãƒšã‚¯ãƒˆãƒ«ç‰¹å¾´é‡ã®åˆ†æ"""
    # ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡å¿ƒï¼ˆéŸ³è‰²ã®æ˜ã‚‹ã•ï¼‰
    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
    
    # ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒ­ãƒ¼ãƒ«ã‚ªãƒ•ï¼ˆé«˜å‘¨æ³¢æˆåˆ†ã®åˆ†å¸ƒï¼‰
    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
    
    # ã‚¼ãƒ­ã‚¯ãƒ­ãƒƒã‚·ãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆï¼ˆéŸ³ã®è³ªæ„Ÿï¼‰
    zcr = librosa.feature.zero_crossing_rate(y)[0]
    
    # MFCCç‰¹å¾´é‡ï¼ˆéŸ³è‰²ç‰¹å¾´ï¼‰
    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    
    # ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯¾æ¯”åº¦ï¼ˆãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯/ãƒ‘ãƒ¼ã‚«ãƒƒã‚·ãƒ–åˆ†é›¢ï¼‰
    harmonic, percussive = librosa.effects.hpss(y)
    harmonic_ratio = np.mean(np.abs(harmonic)) / (np.mean(np.abs(harmonic)) + np.mean(np.abs(percussive)))
    
    return {
        'spectral_centroid_mean': np.mean(spectral_centroids),
        'spectral_centroid_std': np.std(spectral_centroids),
        'spectral_rolloff_mean': np.mean(spectral_rolloff),
        'zcr_mean': np.mean(zcr),
        'mfcc_means': np.mean(mfccs, axis=1),
        'harmonic_ratio': harmonic_ratio
    }

def analyze_dynamics(y, sr, frame_length=2048, hop_length=512):
    """éŸ³é‡å¤‰åŒ–ãƒ»ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹åˆ†æ"""
    # RMS ã‚¨ãƒãƒ«ã‚®ãƒ¼
    rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
    
    # ãƒ‡ã‚·ãƒ™ãƒ«å¤‰æ›
    rms_db = librosa.amplitude_to_db(rms)
    
    # ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒ¬ãƒ³ã‚¸
    dynamic_range = np.max(rms_db) - np.min(rms_db)
    
    # éŸ³é‡å¤‰åŒ–ã®åˆ†æ
    rms_times = librosa.frames_to_time(range(len(rms)), sr=sr, hop_length=hop_length)
    
    # éŸ³é‡ã®ãƒ”ãƒ¼ã‚¯ã‚’æ¤œå‡º
    peaks, _ = scipy.signal.find_peaks(rms, height=np.mean(rms) + np.std(rms))
    peak_times = rms_times[peaks] if len(peaks) > 0 else []
    
    return {
        'rms_mean': np.mean(rms),
        'rms_std': np.std(rms),
        'rms_db_mean': np.mean(rms_db),
        'dynamic_range': dynamic_range,
        'peak_times': peak_times.tolist(),
        'rms_profile': rms.tolist(),
        'rms_times': rms_times.tolist()
    }

def analyze_harmony_and_melody(y, sr):
    """ãƒãƒ¼ãƒ¢ãƒ‹ãƒ¼ã¨ãƒ¡ãƒ­ãƒ‡ã‚£ãƒ¼ãƒ©ã‚¤ãƒ³åˆ†æ"""
    # ã‚¯ãƒ­ãƒç‰¹å¾´é‡ï¼ˆãƒ”ãƒƒãƒã‚¯ãƒ©ã‚¹åˆ†å¸ƒï¼‰
    chroma = librosa.feature.chroma(y=y, sr=sr)
    
    # ãƒˆãƒ‹ãƒƒã‚¯æ¨å®šï¼ˆä¸»èª¿ã®æ¨å®šï¼‰
    chroma_mean = np.mean(chroma, axis=1)
    tonic_estimate = np.argmax(chroma_mean)
    note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']
    estimated_key = note_names[tonic_estimate]
    
    # ãƒ”ãƒƒãƒè¿½è·¡
    pitches, magnitudes = librosa.piptrack(y=y, sr=sr, threshold=0.1)
    
    # ãƒ¡ãƒ­ãƒ‡ã‚£ãƒ¼ãƒ©ã‚¤ãƒ³æŠ½å‡ºã®è©¦è¡Œ
    melody_f0 = []
    for t in range(pitches.shape[1]):
        index = magnitudes[:, t].argmax()
        pitch = pitches[index, t]
        if pitch > 0:
            melody_f0.append(pitch)
        else:
            melody_f0.append(0)
    
    # ãƒ¡ãƒ­ãƒ‡ã‚£ãƒ¼ã®éŸ³ç¨‹å¤‰åŒ–åˆ†æ
    melody_intervals = np.diff([f for f in melody_f0 if f > 0])
    melody_range = max(melody_f0) - min([f for f in melody_f0 if f > 0]) if any(f > 0 for f in melody_f0) else 0
    
    return {
        'estimated_key': estimated_key,
        'chroma_distribution': chroma_mean.tolist(),
        'melody_range_hz': melody_range,
        'melody_f0_profile': melody_f0[:100],  # æœ€åˆã®100ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿
        'harmonic_complexity': np.std(chroma_mean)
    }

def classify_mood_and_emotion(spectral_features, dynamics, harmony, tempo):
    """éŸ³æ¥½ã®é›°å›²æ°—ãƒ»æ„Ÿæƒ…åˆ†æ"""
    # ç‰¹å¾´é‡ã«åŸºã¥ãæ„Ÿæƒ…åˆ†é¡
    energy_level = dynamics['rms_mean']
    brightness = spectral_features['spectral_centroid_mean'] / 4000  # æ­£è¦åŒ–
    harmonic_complexity = harmony['harmonic_complexity']
    
    # æ„Ÿæƒ…åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯
    emotions = {
        'ã‚¨ãƒãƒ«ã‚®ãƒ¼': min(energy_level * 10, 1.0),
        'æ˜ã‚‹ã•': min(brightness, 1.0),
        'è¤‡é›‘ã•': min(harmonic_complexity * 2, 1.0),
        'ãƒ†ãƒ³ãƒæ„Ÿ': min(tempo / 120, 1.0) if tempo > 0 else 0
    }
    
    # ç·åˆçš„ãªé›°å›²æ°—åˆ¤å®š
    if emotions['ã‚¨ãƒãƒ«ã‚®ãƒ¼'] > 0.6 and emotions['ãƒ†ãƒ³ãƒæ„Ÿ'] > 0.8:
        mood = "ã‚¨ãƒãƒ«ã‚®ãƒƒã‚·ãƒ¥ãƒ»æ´»ç™º"
    elif emotions['æ˜ã‚‹ã•'] > 0.6 and emotions['è¤‡é›‘ã•'] < 0.4:
        mood = "æ˜ã‚‹ã„ãƒ»ã‚·ãƒ³ãƒ—ãƒ«"
    elif emotions['ã‚¨ãƒãƒ«ã‚®ãƒ¼'] < 0.4 and emotions['ãƒ†ãƒ³ãƒæ„Ÿ'] < 0.7:
        mood = "é™å¯‚ãƒ»ç‘æƒ³çš„"
    elif emotions['è¤‡é›‘ã•'] > 0.6:
        mood = "è¤‡é›‘ãƒ»æ´—ç·´ã•ã‚ŒãŸ"
    else:
        mood = "ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸãƒ»ä¸­åº¸"
    
    return mood, emotions

def analyze_instrumentation(y, sr, spectral_features):
    """æ¥½å™¨æ§‹æˆãƒ»éŸ³è‰²åˆ†æ"""
    # ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯/ãƒ‘ãƒ¼ã‚«ãƒƒã‚·ãƒ–åˆ†é›¢
    harmonic, percussive = librosa.effects.hpss(y)
    
    # ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯æˆåˆ†ã®å¼·ã•
    harmonic_strength = np.mean(np.abs(harmonic)) / np.mean(np.abs(y))
    
    # ãƒ‘ãƒ¼ã‚«ãƒƒã‚·ãƒ–æˆåˆ†ã®å¼·ã•
    percussive_strength = np.mean(np.abs(percussive)) / np.mean(np.abs(y))
    
    # å‘¨æ³¢æ•°åˆ†æã«ã‚ˆã‚‹æ¥½å™¨æ¨å®š
    spectral_centroid = spectral_features['spectral_centroid_mean']
    harmonic_ratio = spectral_features['harmonic_ratio']
    
    instrument_characteristics = []
    
    # ãƒ”ã‚¢ãƒã®ç‰¹å¾´åˆ¤å®š
    if harmonic_ratio > 0.7 and 200 < spectral_centroid < 2000:
        instrument_characteristics.append("ãƒ”ã‚¢ãƒç³»æ¥½å™¨ã®ç‰¹å¾´")
    
    # å¼¦æ¥½å™¨ã®ç‰¹å¾´åˆ¤å®š
    if harmonic_ratio > 0.8 and spectral_centroid > 1000:
        instrument_characteristics.append("å¼¦æ¥½å™¨ã®ç‰¹å¾´")
    
    # ãƒ‘ãƒ¼ã‚«ãƒƒã‚·ãƒ§ãƒ³åˆ¤å®š
    if percussive_strength > 0.3:
        instrument_characteristics.append("ãƒ‘ãƒ¼ã‚«ãƒƒã‚·ãƒ–ãªè¦ç´ ")
    
    if not instrument_characteristics:
        instrument_characteristics.append("ã‚½ãƒ•ãƒˆãªãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯æ¥½å™¨")
    
    return {
        'harmonic_strength': harmonic_strength,
        'percussive_strength': percussive_strength,
        'instrument_characteristics': instrument_characteristics,
        'tonal_balance': 'ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯é‡è¦–' if harmonic_strength > 0.7 else 'ãƒãƒ©ãƒ³ã‚¹å‹'
    }

def compare_with_plan(analysis_results, duration):
    """è¨ˆç”»ã¨ã®æ¯”è¼ƒ"""
    plan = {
        'target_bpm': (80, 100),
        'target_duration': (30, 40),
        'structure': {
            'intro': 8,
            'development': (12, 16),
            'climax': (8, 10),
            'outro': (6, 8)
        },
        'style': 'ãƒŸãƒ‹ãƒãƒªã‚¹ãƒˆãƒ”ã‚¢ãƒã‚½ãƒ­'
    }
    
    comparison = {}
    
    # BPMæ¯”è¼ƒ
    actual_bpm = analysis_results['tempo']
    if plan['target_bpm'][0] <= actual_bpm <= plan['target_bpm'][1]:
        comparison['bpm_match'] = f"âœ“ ç›®æ¨™ç¯„å›²å†… ({actual_bpm:.1f} BPM)"
    else:
        comparison['bpm_match'] = f"âœ— ç›®æ¨™ç¯„å›²å¤– ({actual_bpm:.1f} BPM, ç›®æ¨™: {plan['target_bpm'][0]}-{plan['target_bpm'][1]})"
    
    # é•·ã•æ¯”è¼ƒ
    if plan['target_duration'][0] <= duration <= plan['target_duration'][1]:
        comparison['duration_match'] = f"âœ“ ç›®æ¨™ç¯„å›²å†… ({duration:.1f}ç§’)"
    else:
        comparison['duration_match'] = f"âœ— ç›®æ¨™ç¯„å›²å¤– ({duration:.1f}ç§’, ç›®æ¨™: {plan['target_duration'][0]}-{plan['target_duration'][1]}ç§’)"
    
    # ã‚¹ã‚¿ã‚¤ãƒ«æ¯”è¼ƒ
    instruments = analysis_results['instrumentation']['instrument_characteristics']
    if any('ãƒ”ã‚¢ãƒ' in inst for inst in instruments):
        comparison['style_match'] = "âœ“ ãƒ”ã‚¢ãƒç³»æ¥½å™¨ã®ç‰¹å¾´ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ"
    else:
        comparison['style_match'] = "? ãƒ”ã‚¢ãƒç‰¹æœ‰ã®ç‰¹å¾´ãŒæ˜ç¢ºã§ã¯ã‚ã‚Šã¾ã›ã‚“"
    
    return comparison

def generate_visual_suggestions(analysis_results, mood, duration):
    """è¦–è¦šçš„è¦ç´ ã¸ã®ç¤ºå”†"""
    suggestions = {
        'color_palette': [],
        'visual_rhythm': '',
        'camera_movement': '',
        'lighting': '',
        'texture_style': ''
    }
    
    # è‰²å½©ãƒ‘ãƒ¬ãƒƒãƒˆææ¡ˆ
    brightness = analysis_results['spectral_features']['spectral_centroid_mean']
    energy = analysis_results['dynamics']['rms_mean']
    
    if brightness > 1500:
        suggestions['color_palette'].extend(['æ˜ã‚‹ã„ç™½', 'ã‚¯ãƒªãƒ¼ãƒ è‰²', 'æ·¡ã„ã‚´ãƒ¼ãƒ«ãƒ‰'])
    elif brightness > 800:
        suggestions['color_palette'].extend(['ã‚½ãƒ•ãƒˆã‚°ãƒ¬ãƒ¼', 'æ¸©ã‹ã„ãƒ™ãƒ¼ã‚¸ãƒ¥', 'æ·¡ã„ãƒ–ãƒ«ãƒ¼'])
    else:
        suggestions['color_palette'].extend(['æ·±ã„ã‚°ãƒ¬ãƒ¼', 'ãƒŸãƒƒãƒ‰ãƒŠã‚¤ãƒˆãƒ–ãƒ«ãƒ¼', 'è½ã¡ç€ã„ãŸãƒ–ãƒ©ã‚¦ãƒ³'])
    
    # è¦–è¦šçš„ãƒªã‚ºãƒ 
    tempo = analysis_results['tempo']
    if tempo < 80:
        suggestions['visual_rhythm'] = 'ã‚†ã£ãã‚Šã¨ã—ãŸã€ç‘æƒ³çš„ãªå‹•ã'
    elif tempo < 100:
        suggestions['visual_rhythm'] = 'ç©ã‚„ã‹ã§æµã‚Œã‚‹ã‚ˆã†ãªå‹•ã'
    else:
        suggestions['visual_rhythm'] = 'ãƒªã‚ºãƒŸã‚«ãƒ«ã§æ´»ç™ºãªå‹•ã'
    
    # ã‚«ãƒ¡ãƒ©å‹•ä½œ
    dynamic_range = analysis_results['dynamics']['dynamic_range']
    if dynamic_range > 20:
        suggestions['camera_movement'] = 'ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãªã‚ºãƒ¼ãƒ ã¨ãƒ‘ãƒ³'
    elif dynamic_range > 10:
        suggestions['camera_movement'] = 'æ»‘ã‚‰ã‹ãªã‚¹ãƒ©ã‚¤ãƒ‰ã¨ãƒ†ã‚£ãƒ«ãƒˆ'
    else:
        suggestions['camera_movement'] = 'é™çš„ã¾ãŸã¯éå¸¸ã«ç©ã‚„ã‹ãªå‹•ã'
    
    # ç…§æ˜ã‚¹ã‚¿ã‚¤ãƒ«
    harmonic_ratio = analysis_results['spectral_features']['harmonic_ratio']
    if harmonic_ratio > 0.8:
        suggestions['lighting'] = 'ã‚½ãƒ•ãƒˆã§å‡ä¸€ãªç…§æ˜ã€æ¸©ã‹ã„è‰²æ¸©åº¦'
    else:
        suggestions['lighting'] = 'ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã®ã‚ã‚‹ç…§æ˜ã€é™°å½±ã‚’åŠ¹ã‹ã›ãŸæ¼”å‡º'
    
    # ãƒ†ã‚¯ã‚¹ãƒãƒ£ã‚¹ã‚¿ã‚¤ãƒ«
    if mood in ['é™å¯‚ãƒ»ç‘æƒ³çš„', 'ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸãƒ»ä¸­åº¸']:
        suggestions['texture_style'] = 'ãƒŸãƒ‹ãƒãƒ«ã§æ¸…æ½”ã€å¾®ç´°ãªãƒ†ã‚¯ã‚¹ãƒãƒ£'
    else:
        suggestions['texture_style'] = 'ãƒªãƒƒãƒã§ãƒ‡ã‚£ãƒ†ãƒ¼ãƒ«ã®ã‚ã‚‹ãƒ†ã‚¯ã‚¹ãƒãƒ£'
    
    return suggestions

def main():
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    audio_file = "/home/runner/work/kamuicode-workflow/kamuicode-workflow/music-video-20250721-16406899626/music/generated-music.wav"
    
    print("éŸ³æ¥½åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"åˆ†æãƒ•ã‚¡ã‚¤ãƒ«: {audio_file}")
    
    # éŸ³æ¥½ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    try:
        y, sr = librosa.load(audio_file, sr=None)
        duration = librosa.get_duration(y=y, sr=sr)
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº† - é•·ã•: {duration:.2f}ç§’, ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: {sr}Hz")
    except Exception as e:
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # å„ç¨®åˆ†æå®Ÿè¡Œ
    print("\n=== è©³ç´°åˆ†æå®Ÿè¡Œä¸­ ===")
    
    # 1. æ§‹é€ åˆ†æ
    print("1. éŸ³æ¥½æ§‹é€ åˆ†æ...")
    structure_boundaries = analyze_music_structure(y, sr)
    
    # 2. ãƒ†ãƒ³ãƒãƒ»ãƒªã‚ºãƒ åˆ†æ
    print("2. ãƒ†ãƒ³ãƒãƒ»ãƒªã‚ºãƒ åˆ†æ...")
    tempo, beat_times, rhythm_stability = analyze_tempo_and_rhythm(y, sr)
    
    # 3. ã‚¹ãƒšã‚¯ãƒˆãƒ«ç‰¹å¾´é‡åˆ†æ
    print("3. ã‚¹ãƒšã‚¯ãƒˆãƒ«ç‰¹å¾´é‡åˆ†æ...")
    spectral_features = analyze_spectral_features(y, sr)
    
    # 4. ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹åˆ†æ
    print("4. ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹åˆ†æ...")
    dynamics = analyze_dynamics(y, sr)
    
    # 5. ãƒãƒ¼ãƒ¢ãƒ‹ãƒ¼ãƒ»ãƒ¡ãƒ­ãƒ‡ã‚£ãƒ¼åˆ†æ
    print("5. ãƒãƒ¼ãƒ¢ãƒ‹ãƒ¼ãƒ»ãƒ¡ãƒ­ãƒ‡ã‚£ãƒ¼åˆ†æ...")
    harmony = analyze_harmony_and_melody(y, sr)
    
    # 6. æ¥½å™¨æ§‹æˆåˆ†æ
    print("6. æ¥½å™¨æ§‹æˆåˆ†æ...")
    instrumentation = analyze_instrumentation(y, sr, spectral_features)
    
    # 7. æ„Ÿæƒ…ãƒ»é›°å›²æ°—åˆ†æ
    print("7. æ„Ÿæƒ…ãƒ»é›°å›²æ°—åˆ†æ...")
    mood, emotions = classify_mood_and_emotion(spectral_features, dynamics, harmony, tempo)
    
    # 8. è¨ˆç”»ã¨ã®æ¯”è¼ƒ
    print("8. è¨ˆç”»ã¨ã®æ¯”è¼ƒ...")
    comparison = compare_with_plan({
        'tempo': tempo,
        'instrumentation': instrumentation,
        'spectral_features': spectral_features,
        'dynamics': dynamics
    }, duration)
    
    # 9. è¦–è¦šçš„è¦ç´ ææ¡ˆ
    print("9. è¦–è¦šçš„è¦ç´ ææ¡ˆç”Ÿæˆ...")
    visual_suggestions = generate_visual_suggestions({
        'spectral_features': spectral_features,
        'dynamics': dynamics,
        'tempo': tempo
    }, mood, duration)
    
    # çµæœã®ã¾ã¨ã‚
    analysis_results = {
        'file_info': {
            'duration_seconds': duration,
            'sample_rate': sr,
            'channels': 'ã‚¹ãƒ†ãƒ¬ã‚ª',
            'bit_depth': '16-bit'
        },
        'structure': {
            'total_duration': duration,
            'boundaries': structure_boundaries.tolist(),
            'estimated_sections': len(structure_boundaries) - 1
        },
        'tempo_rhythm': {
            'bpm': tempo,
            'rhythm_stability': rhythm_stability,
            'beat_count': len(beat_times)
        },
        'spectral_features': spectral_features,
        'dynamics': dynamics,
        'harmony': harmony,
        'instrumentation': instrumentation,
        'mood_emotion': {
            'overall_mood': mood,
            'emotion_scores': emotions
        },
        'plan_comparison': comparison,
        'visual_suggestions': visual_suggestions,
        'analysis_timestamp': datetime.now().isoformat()
    }
    
    # çµæœå‡ºåŠ›
    print("\n" + "="*60)
    print("ğŸµ éŸ³æ¥½åˆ†æçµæœãƒ¬ãƒãƒ¼ãƒˆ")
    print("="*60)
    
    print(f"\nğŸ“Š åŸºæœ¬æƒ…å ±:")
    print(f"  é•·ã•: {duration:.2f}ç§’")
    print(f"  ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: {sr}Hz")
    print(f"  ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: 16-bit ã‚¹ãƒ†ãƒ¬ã‚ª WAV")
    
    print(f"\nğŸ—ï¸ éŸ³æ¥½æ§‹é€ :")
    print(f"  æ¨å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(structure_boundaries)-1}")
    print(f"  ã‚»ã‚¯ã‚·ãƒ§ãƒ³å¢ƒç•Œ: {[f'{t:.1f}s' for t in structure_boundaries]}")
    
    print(f"\nğŸ¥ ãƒ†ãƒ³ãƒãƒ»ãƒªã‚ºãƒ :")
    print(f"  BPM: {tempo:.1f}")
    print(f"  ãƒªã‚ºãƒ å®‰å®šæ€§: {rhythm_stability:.3f}")
    print(f"  æ¤œå‡ºãƒ“ãƒ¼ãƒˆæ•°: {len(beat_times)}")
    
    print(f"\nğŸ¼ æ¥½å™¨æ§‹æˆãƒ»éŸ³è‰²:")
    print(f"  ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯å¼·åº¦: {instrumentation['harmonic_strength']:.3f}")
    print(f"  ãƒ‘ãƒ¼ã‚«ãƒƒã‚·ãƒ–å¼·åº¦: {instrumentation['percussive_strength']:.3f}")
    print(f"  æ¥½å™¨ç‰¹å¾´: {', '.join(instrumentation['instrument_characteristics'])}")
    print(f"  éŸ³è‰²ãƒãƒ©ãƒ³ã‚¹: {instrumentation['tonal_balance']}")
    
    print(f"\nğŸ­ éŸ³æ¥½ã®é›°å›²æ°—ãƒ»æ„Ÿæƒ…:")
    print(f"  ç·åˆçš„é›°å›²æ°—: {mood}")
    print(f"  æ„Ÿæƒ…ã‚¹ã‚³ã‚¢:")
    for emotion, score in emotions.items():
        print(f"    {emotion}: {score:.3f}")
    
    print(f"\nğŸ“ˆ éŸ³é‡å¤‰åŒ–ãƒ»ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹:")
    print(f"  å¹³å‡éŸ³é‡(RMS): {dynamics['rms_mean']:.4f}")
    print(f"  ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒ¬ãƒ³ã‚¸: {dynamics['dynamic_range']:.1f}dB")
    print(f"  éŸ³é‡ãƒ”ãƒ¼ã‚¯æ•°: {len(dynamics['peak_times'])}")
    
    print(f"\nğŸ¶ éŸ³æ¥½çš„ç‰¹å¾´:")
    print(f"  æ¨å®šèª¿æ€§: {harmony['estimated_key']}")
    print(f"  ãƒ¡ãƒ­ãƒ‡ã‚£ãƒ¼éŸ³åŸŸ: {harmony['melody_range_hz']:.1f}Hz")
    print(f"  ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯è¤‡é›‘ã•: {harmony['harmonic_complexity']:.3f}")
    print(f"  ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡å¿ƒ: {spectral_features['spectral_centroid_mean']:.1f}Hz")
    
    print(f"\nğŸ“‹ è¨ˆç”»ã¨ã®æ¯”è¼ƒ:")
    for key, value in comparison.items():
        print(f"  {key}: {value}")
    
    print(f"\nğŸ¨ è¦–è¦šçš„è¦ç´ ã¸ã®ç¤ºå”†:")
    print(f"  æ¨å¥¨è‰²å½©: {', '.join(visual_suggestions['color_palette'])}")
    print(f"  è¦–è¦šçš„ãƒªã‚ºãƒ : {visual_suggestions['visual_rhythm']}")
    print(f"  ã‚«ãƒ¡ãƒ©å‹•ä½œ: {visual_suggestions['camera_movement']}")
    print(f"  ç…§æ˜ã‚¹ã‚¿ã‚¤ãƒ«: {visual_suggestions['lighting']}")
    print(f"  ãƒ†ã‚¯ã‚¹ãƒãƒ£: {visual_suggestions['texture_style']}")
    
    # JSONå½¢å¼ã§ã‚‚ä¿å­˜
    output_file = "/home/runner/work/kamuicode-workflow/kamuicode-workflow/music-video-20250721-16406899626/analysis/detailed_music_analysis_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è©³ç´°çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜: {output_file}")
    print("\n" + "="*60)
    print("åˆ†æå®Œäº†ï¼")
    
    return analysis_results

if __name__ == "__main__":
    main()